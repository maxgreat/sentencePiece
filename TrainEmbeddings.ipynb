{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SentencePiece\n",
    "découpe les mots. Peut être utilisé en multi-langue (particulièrement pour les langues avec des bases de mots communes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordEmbeddings \n",
    "Normalement appliqué après tokenization. Doit être appris sur une langue. Serait-il possible d’apprendre des sentencePice embedding dans plusieurs langue? A tester sur une langue, puis deux, puis 30 (comme MUSE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectifs : \n",
    "* Apprendre sentencePiece\n",
    "* Apprendre sentencePiece Embeddings\n",
    "* Comment évaluer les embeddings?\n",
    "* Apprendre sentencePiece Embeddings en plusieurs langues\n",
    "* Evaluation?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id = {\"hello\": 0, \"world\": 1} # create dictionnary of words ids\n",
    "embeds = nn.Embedding(len(word_to_id), embed_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0988,  1.3208,  1.4393,  0.7731, -0.3379, -1.2744, -0.4319, -0.5081,\n",
      "         -0.3132,  1.8581]], grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "word_tensor = torch.tensor([word_to_id[\"hello\"]], dtype=torch.long)\n",
    "hello_embed = embeds(word_tensor)\n",
    "print(hello_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained Embeddings with CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Deep learning (also known as deep structured learning or hierarchical learning) \n",
    "is part of a broader family of machine learning methods based on learning data representations, \n",
    "as opposed to task-specific algorithms. Learning can be supervised, semi-supervised or unsupervised. \n",
    "Deep learning architectures such as deep neural networks, deep belief networks and recurrent neural \n",
    "networks have been applied to fields including computer vision, speech recognition, natural language \n",
    "processing, audio recognition, social network filtering, machine translation, bioinformatics, \n",
    "drug design, medical image analysis, material inspection and board game programs, where they have \n",
    "produced results comparable to and in some cases superior to human experts. Deep learning models are \n",
    "vaguely inspired by information processing and communication patterns in biological nervous systems \n",
    "yet have various differences from the structural and functional properties of biological brains \n",
    "(especially human brains), which make them incompatible with neuroscience evidences.\"\"\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(text)\n",
    "vocab_size = len(vocab)\n",
    "word2id = {word:i for i,word in enumerate(vocab)}\n",
    "id2word = {i:word for i,word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['Deep', 'learning', 'known', 'as'], '(also'), (['learning', '(also', 'as', 'deep'], 'known'), (['(also', 'known', 'deep', 'structured'], 'as'), (['known', 'as', 'structured', 'learning'], 'deep'), (['as', 'deep', 'learning', 'or'], 'structured')]\n"
     ]
    }
   ],
   "source": [
    "datas = []\n",
    "for i in range(2, len(text) - 2):\n",
    "    context = [text[i - 2], text[i - 1],\n",
    "               text[i + 1], text[i + 2]]\n",
    "    target = text[i]\n",
    "    datas.append((context, target))\n",
    "print(datas[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_context_vector(context, word_to_ix):\n",
    "    idxs = [word_to_ix[w] for w in context]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_prob_result(input, ix_to_word):\n",
    "    return ix_to_word[get_index_of_max(input)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_of_max(input):\n",
    "    index = 0\n",
    "    for i in range(1, len(input)):\n",
    "        if input[i] > input[index]:\n",
    "            index = i \n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size=128):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.projection = nn.Sequential(\n",
    "                            nn.Linear(embedding_dim, hidden_size),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Linear(hidden_size, vocab_size),\n",
    "                            nn.LogSoftmax(dim = -1)\n",
    "                        )\n",
    "        \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = sum(self.embeddings(inputs)).view(1,-1)\n",
    "        out = self.projection(embeds)\n",
    "        return out\n",
    "\n",
    "    def get_word_emdedding(self, word):\n",
    "        word = torch.LongTensor([word2id[word]])\n",
    "        return self.embeddings(word).view(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CBOW(vocab_size, embed_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(50):\n",
    "    total_loss = 0\n",
    "    for context, target in datas:\n",
    "        context_vector = make_context_vector(context, word2id)  \n",
    "        model.zero_grad()\n",
    "        log_probs = model(context_vector)\n",
    "        loss = loss_function(log_probs, torch.tensor([word2id[target]], dtype=torch.long))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw text: Deep learning (also known as deep structured learning or hierarchical learning) is part of a broader family of machine learning methods based on learning data representations, as opposed to task-specific algorithms. Learning can be supervised, semi-supervised or unsupervised. Deep learning architectures such as deep neural networks, deep belief networks and recurrent neural networks have been applied to fields including computer vision, speech recognition, natural language processing, audio recognition, social network filtering, machine translation, bioinformatics, drug design, medical image analysis, material inspection and board game programs, where they have produced results comparable to and in some cases superior to human experts. Deep learning models are vaguely inspired by information processing and communication patterns in biological nervous systems yet have various differences from the structural and functional properties of biological brains (especially human brains), which make them incompatible with neuroscience evidences.\n",
      "\n",
      "Context: ['deep', 'networks']\n",
      "\n",
      "Prediction: as\n"
     ]
    }
   ],
   "source": [
    "context = ['deep','networks']\n",
    "context_vector = make_context_vector(context, word2id)\n",
    "a = model(context_vector).data.numpy()\n",
    "print('Raw text: {}\\n'.format(' '.join(text)))\n",
    "print('Context: {}\\n'.format(context))\n",
    "print('Prediction: {}'.format(get_max_prob_result(a[0], id2word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train embeddings with NGram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ngram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, embeddings, context_size, hidden_size=128):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = embeddings\n",
    "        self.linear1 = nn.Linear(context_size * embeddings.embedding_dim, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_size=4\n",
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = NGramLanguageModeler(model.embeddings, context_size)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[638.649551153183, 634.9874835014343, 631.3503413200378, 627.7379004955292, 624.1504530906677, 620.5849575996399, 617.0376613140106, 613.5099146366119, 610.0003879070282, 606.5009481906891]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for context, target in datas:\n",
    "\n",
    "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "        # into integer indices and wrap them in tensors)\n",
    "        context_idxs = torch.tensor([word2id[w] for w in context], dtype=torch.long)\n",
    "\n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "        # new instance, you need to zero out the gradients from the old\n",
    "        # instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        log_probs = model(context_idxs)\n",
    "\n",
    "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a tensor)\n",
    "        loss = loss_function(log_probs, torch.tensor([word2id[target]], dtype=torch.long))\n",
    "\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "        total_loss += loss.item()\n",
    "    losses.append(total_loss)\n",
    "print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw text: Deep learning (also known as deep structured learning or hierarchical learning) is part of a broader family of machine learning methods based on learning data representations, as opposed to task-specific algorithms. Learning can be supervised, semi-supervised or unsupervised. Deep learning architectures such as deep neural networks, deep belief networks and recurrent neural networks have been applied to fields including computer vision, speech recognition, natural language processing, audio recognition, social network filtering, machine translation, bioinformatics, drug design, medical image analysis, material inspection and board game programs, where they have produced results comparable to and in some cases superior to human experts. Deep learning models are vaguely inspired by information processing and communication patterns in biological nervous systems yet have various differences from the structural and functional properties of biological brains (especially human brains), which make them incompatible with neuroscience evidences.\n",
      "\n",
      "Context: ['deep', 'methods', 'of', 'is']\n",
      "\n",
      "Prediction: various\n"
     ]
    }
   ],
   "source": [
    "context = ['deep','methods','of', 'is']\n",
    "context_vector = make_context_vector(context, word2id)\n",
    "a = model(context_vector).data.numpy()\n",
    "print('Raw text: {}\\n'.format(' '.join(text)))\n",
    "print('Context: {}\\n'.format(context))\n",
    "print('Prediction: {}'.format(get_max_prob_result(a[0], id2word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SkipGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dimension):\n",
    "        \"\"\"Initialize model parameters. \n",
    "        Args: \n",
    "            vocab_size: vocab size. \n",
    "            emb_dimention: Embedding dimention, typically from 50 to 500. \"\"\"\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.emb_size = vocab_size\n",
    "        self.emb_dimension = emb_dimension\n",
    "        self.u_embeddings = nn.Embedding(vocab_size, emb_dimension, sparse=True)\n",
    "        self.v_embeddings = nn.Embedding(vocab_size, emb_dimension, sparse=True)\n",
    "        self.init_emb()\n",
    "        \n",
    "    def init_emb(self):\n",
    "        initrange = 0.5 / self.emb_dimension\n",
    "        self.u_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        self.v_embeddings.weight.data.uniform_(-0, 0)\n",
    "        \n",
    "    def forward(self, pos_u, pos_v, neg_v):\n",
    "        \"\"\" \n",
    "        Args: \n",
    "            pos_u: list of center word ids for positive word pairs. \n",
    "            pos_v: list of neibor word ids for positive word pairs. \n",
    "            neg_v: list of neibor word ids for negative word pairs. \n",
    "        \"\"\"\n",
    "        losses = []\n",
    "        emb_u = self.u_embeddings(pos_u)\n",
    "        emb_v = self.v_embeddings(pos_v)\n",
    "        score = torch.mul(emb_u, emb_v).squeeze()\n",
    "        score = torch.sum(score, dim=1)\n",
    "        score = F.logsigmoid(score)\n",
    "        losses.append(sum(score))\n",
    "        neg_emb_v = self.v_embeddings(neg_v)\n",
    "        neg_score = torch.bmm(neg_emb_v, emb_u.unsqueeze(2)).squeeze(2)\n",
    "        neg_score = torch.sum(neg_score, dim=1)\n",
    "        neg_score = F.logsigmoid(-1 * neg_score)\n",
    "        losses.append(sum(neg_score))\n",
    "        return -1 * sum(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = nn.Embedding(100, 10, sparse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.LongTensor(1)\n",
    "t[0] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e(t).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "class contextDataset(data.Dataset):\n",
    "    def __init__(self, text, word2id):\n",
    "        self.text = text\n",
    "        self.word2id = word2id\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(text)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        target = torch.LongTensor([self.word2id[self.text[i]]])\n",
    "        if i >= 2 and i < len(text) - 2:\n",
    "            context = [self.text[i - 2], self.text[i - 1],\n",
    "                      self.text[i + 1], self.text[i + 2]]\n",
    "        elif i == 1:\n",
    "            context = [self.text[i - 1], self.text[i + 1],\n",
    "                      self.text[i + 2], self.text[i + 3]]\n",
    "        elif i == 0:\n",
    "            context = [self.text[i + 1], self.text[i + 2], \n",
    "                       self.text[i + 3], self.text[i+4]]\n",
    "        elif i >= len(text) - 2:\n",
    "            context = [self.text[i-4], self.text[i-3] ,self.text[i - 2], self.text[i - 1]]\n",
    "        else:\n",
    "            print(\"ERROOR :\", i, '/', len(self.text))\n",
    "            return None\n",
    "        context = [self.word2id[c] for c in context]\n",
    "        \n",
    "        neg = random.choices(list(word2id.values()), k=1)\n",
    "        \n",
    "        return target, context, neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd = contextDataset(text, word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2]), [19, 65, 12, 8], [96])"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cd[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SkipGramModel(vocab_size, embed_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = torch.LongTensor(1)\n",
    "i[0] = cd[0][0]\n",
    "p = torch.LongTensor(1,4)\n",
    "p[0] = torch.LongTensor(cd[0][1])\n",
    "n = torch.LongTensor(1,5)\n",
    "n[0] = torch.LongTensor(cd[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.4657, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(i,p,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[638.649551153183, 634.9874835014343, 631.3503413200378, 627.7379004955292, 624.1504530906677, 620.5849575996399, 617.0376613140106, 613.5099146366119, 610.0003879070282, 606.5009481906891, 722.7742567062378, 722.7742567062378, 722.7742567062378, 722.7742567062378, 722.7742567062378, 722.7742567062378, 722.7742567062378, 722.7742567062378, 722.7742567062378, 722.7742567062378]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for target, context, neg in cd:\n",
    "        model.zero_grad()\n",
    "        \n",
    "        cont = torch.LongTensor(1,len(context))\n",
    "        cont[0] = torch.LongTensor(context)\n",
    "        \n",
    "        n = torch.LongTensor(1,len(neg))\n",
    "        n[0] = torch.LongTensor(neg)\n",
    "        \n",
    "        log_probs = model(target, cont, n)\n",
    "\n",
    "        log_probs.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    losses.append(total_loss)\n",
    "print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
